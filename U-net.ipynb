{"cells":[{"cell_type":"markdown","metadata":{"id":"2Yz0UuyXWZ13"},"source":["Load file"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"VPNCt7LSWZ15"},"outputs":[],"source":["import os\n","from pathlib import Path\n","from typing import Dict, Optional, Union\n","\n","import numpy as np\n","import pandas as pd\n","from numpy.typing import NDArray\n","\n","\n","def load_file(file_path: Path) -> Union[NDArray, Dict[str, NDArray]]:\n","    \"\"\"\n","    Load a file in one of the formats provided in the OpenKBP dataset\n","    \"\"\"\n","    if file_path.stem == \"voxel_dimensions\":\n","        return np.loadtxt(file_path)\n","\n","    loaded_file_df = pd.read_csv(file_path, index_col=0)\n","    if loaded_file_df.isnull().values.any():  # Data is a mask\n","        loaded_file = np.array(loaded_file_df.index).squeeze()\n","    else:  # Data is a sparse matrix\n","        loaded_file = {\"indices\": loaded_file_df.index.values, \"data\": loaded_file_df.data.values}\n","\n","    return loaded_file\n","\n","\n","def get_paths(directory_path: Path, extension: Optional[str] = None) -> list[Path]:\n","    \"\"\"\n","    Get the paths of every file contained in `directory_path` that also has the extension `extension` if one is provided.\n","    \"\"\"\n","    all_paths = []\n","\n","    if not directory_path.is_dir():\n","        pass\n","    elif extension is None:\n","        dir_list = os.listdir(directory_path)\n","        for name in dir_list:\n","            if \".\" != name[0]:  # Ignore hidden files\n","                all_paths.append(directory_path / str(name))\n","    else:\n","        data_root = Path(directory_path)\n","        for file_path in data_root.glob(\"*.{}\".format(extension)):\n","            file_path = Path(file_path)\n","            if \".\" != file_path.stem[0]:\n","                all_paths.append(file_path)\n","\n","    return all_paths\n","\n","\n","def sparse_vector_function(x, indices=None) -> dict[str, NDArray]:\n","    \"\"\"Convert a tensor into a dictionary of the non-zero values and their corresponding indices\n","    :param x: the tensor or, if indices is not None, the values that belong at each index\n","    :param indices: the raveled indices of the tensor\n","    :return:  sparse vector in the form of a dictionary\n","    \"\"\"\n","    if indices is None:\n","        y = {\"data\": x[x > 0], \"indices\": np.nonzero(x.flatten())[-1]}\n","    else:\n","        y = {\"data\": x[x > 0], \"indices\": indices[x > 0]}\n","    return y\n"]},{"cell_type":"markdown","metadata":{"id":"6JbKK-DDWZ16"},"source":["Data Shpae"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"66WgBZYPWZ16"},"outputs":[],"source":["from typing import Union\n","\n","from numpy.typing import NDArray\n","\n","\n","class DataShapes:\n","    def __init__(self, num_rois):\n","        self.num_rois = num_rois\n","        self.patient_shape = (128, 128, 128)\n","\n","    @property\n","    def dose(self) -> tuple[int, int, int, int]:\n","        \"\"\"Dose deposited within the patient tensor\"\"\"\n","        return self.patient_shape + (1,)\n","\n","    @property\n","    def predicted_dose(self) -> tuple[int, int, int, int]:\n","        \"\"\"Predicted dose that should be deposited within the patient tensor\"\"\"\n","        return self.dose\n","\n","    @property\n","    def ct(self) -> tuple[int, int, int, int]:\n","        \"\"\"CT image grey scale within the patient tensor\"\"\"\n","        return self.patient_shape + (1,)\n","\n","    @property\n","    def structure_masks(self) -> tuple[int, int, int, int]:\n","        \"\"\"Mask of all structures in patient\"\"\"\n","        return self.patient_shape + (self.num_rois,)\n","\n","    @property\n","    def possible_dose_mask(self) -> tuple[int, int, int, int]:\n","        \"\"\"Mask where dose can be deposited\"\"\"\n","        return self.patient_shape + (1,)\n","\n","    @property\n","    def voxel_dimensions(self) -> tuple[float]:\n","        \"\"\"Physical dimensions of patient voxels (in mm)\"\"\"\n","        return tuple((3,))\n","\n","    def from_data_names(self, data_names: list[str]) -> dict[str, Union[NDArray, tuple[float]]]:\n","        data_shapes = {}\n","        for name in data_names:\n","            data_shapes[name] = getattr(self, name)\n","        return data_shapes\n"]},{"cell_type":"markdown","metadata":{"id":"xwNMdXYCWZ17"},"source":["Batch Data\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0tPSv7eqWZ17"},"outputs":[],"source":["from __future__ import annotations\n","\n","from pathlib import Path\n","from typing import Optional\n","\n","import numpy as np\n","from numpy.typing import NDArray\n","\n","\n","class DataBatch:\n","    def __init__(\n","        self,\n","        dose: Optional[NDArray] = None,\n","        predicted_dose: Optional[NDArray] = None,\n","        ct: Optional[NDArray] = None,\n","        structure_masks: Optional[NDArray] = None,\n","        structure_mask_names: Optional[list[str]] = None,\n","        possible_dose_mask: Optional[NDArray] = None,\n","        voxel_dimensions: Optional[NDArray] = None,\n","        patient_list: Optional[list[str]] = None,\n","        patient_path_list: Optional[list[Path]] = None,\n","    ):\n","        self.dose = dose\n","        self.predicted_dose = predicted_dose\n","        self.ct = ct\n","        self.structure_masks = structure_masks\n","        self.structure_mask_names = structure_mask_names\n","        self.possible_dose_mask = possible_dose_mask\n","        self.voxel_dimensions = voxel_dimensions\n","        self.patient_list = patient_list\n","        self.patient_path = patient_path_list\n","\n","    @classmethod\n","    def initialize_from_required_data(cls, data_dimensions: dict[str, NDArray], batch_size: int) -> DataBatch:\n","        attribute_values = {}\n","        for data, dimensions in data_dimensions.items():\n","            batch_data_dimensions = (batch_size, *dimensions)\n","            attribute_values[data] = np.zeros(batch_data_dimensions)\n","        return cls(**attribute_values)\n","\n","    def set_values(self, data_name: str, batch_index: int, values: NDArray):\n","        getattr(self, data_name)[batch_index] = values\n","\n","    def get_index_structure_from_structure(self, structure_name: str):\n","        return self.structure_mask_names.index(structure_name)\n"]},{"cell_type":"markdown","metadata":{"id":"aP0EW1nrWZ17"},"source":["Data Load"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gUiuVAsbWZ17"},"outputs":[],"source":["from pathlib import Path\n","from typing import Dict, Iterator, List, Optional, Union\n","\n","import numpy as np\n","from more_itertools import windowed\n","from numpy.typing import NDArray\n","from tqdm import tqdm\n","\n","\n","class DataLoader:\n","    \"\"\"Loads OpenKBP csv data in structured format for dose prediction models.\"\"\"\n","\n","    def __init__(self, patient_paths: List[Path], batch_size: int = 2):\n","        \"\"\"\n","        :param patient_paths: list of the paths where data for each patient is stored\n","        :param batch_size: the number of data points to lead in a single batch\n","        \"\"\"\n","        self.patient_paths = patient_paths\n","        self.batch_size = batch_size\n","\n","        # Light processing of attributes\n","        self.paths_by_patient_id = {patient_path.stem: patient_path for patient_path in self.patient_paths}\n","        self.required_files: Optional[Dict] = None\n","        self.mode_name: Optional[str] = None\n","\n","        # Parameters that should not be changed unless OpenKBP data is modified\n","        self.rois = dict(\n","            oars=[\"Brainstem\", \"SpinalCord\", \"RightParotid\", \"LeftParotid\", \"Esophagus\", \"Larynx\", \"Mandible\"],\n","            targets=[\"PTV56\", \"PTV63\", \"PTV70\"],\n","        )\n","        self.full_roi_list = sum(map(list, self.rois.values()), [])  # make a list of all rois\n","        self.num_rois = len(self.full_roi_list)\n","        self.data_shapes = DataShapes(self.num_rois)\n","\n","    @property\n","    def patient_id_list(self) -> List[str]:\n","        return list(self.paths_by_patient_id.keys())\n","\n","    def get_batches(self) -> Iterator[DataBatch]:\n","        batches = windowed(self.patient_paths, n=self.batch_size, step=self.batch_size)\n","        complete_batches = (batch for batch in batches if None not in batch)\n","        for batch_paths in tqdm(complete_batches):\n","            yield self.prepare_data(batch_paths)\n","\n","    def get_patients(self, patient_list: List[str]) -> DataBatch:\n","        file_paths_to_load = [self.paths_by_patient_id[patient] for patient in patient_list]\n","        return self.prepare_data(file_paths_to_load)\n","\n","    def set_mode(self, mode: str) -> None:\n","        \"\"\"Set parameters based on `mode`.\"\"\"\n","        self.mode_name = mode\n","        if mode == \"training_model\":\n","            required_data = [\"dose\", \"ct\", \"structure_masks\", \"possible_dose_mask\", \"voxel_dimensions\"]\n","        elif mode == \"predicted_dose\":\n","            required_data = [mode]\n","            self._force_batch_size_one()\n","        elif mode == \"evaluation\":\n","            required_data = [\"dose\", \"structure_masks\", \"possible_dose_mask\", \"voxel_dimensions\"]\n","            self._force_batch_size_one()\n","        elif mode == \"dose_prediction\":\n","            required_data = [\"ct\", \"structure_masks\", \"possible_dose_mask\", \"voxel_dimensions\"]\n","            self._force_batch_size_one()\n","        else:\n","            raise ValueError(f\"Mode `{mode}` does not exist. Mode must be either training_model, prediction, predicted_dose, or evaluation\")\n","        self.required_files = self.data_shapes.from_data_names(required_data)\n","\n","    def _force_batch_size_one(self) -> None:\n","        if self.batch_size != 1:\n","            self.batch_size = 1\n","            Warning(\"Batch size has been changed to 1 for dose prediction mode\")\n","\n","    def shuffle_data(self) -> None:\n","        np.random.shuffle(self.patient_paths)\n","\n","    def prepare_data(self, file_paths_to_load: List[Path]) -> DataBatch:\n","        \"\"\"Prepares data containing samples in batch so that they are loaded in the proper shape: (n_samples, *dim, n_channels)\"\"\"\n","\n","        batch_data = DataBatch.initialize_from_required_data(self.required_files, self.batch_size)\n","        batch_data.patient_list = [patient_path.stem for patient_path in file_paths_to_load]\n","        batch_data.patient_path_list = file_paths_to_load\n","        batch_data.structure_mask_names = self.full_roi_list\n","\n","        # Populate batch with requested data\n","        for index, patient_path in enumerate(file_paths_to_load):\n","            raw_data = self.load_data(patient_path)\n","            for key in self.required_files:\n","                batch_data.set_values(key, index, self.shape_data(key, raw_data))\n","\n","        return batch_data\n","\n","    def load_data(self, path_to_load: Path) -> Union[NDArray, dict[str, NDArray]]:\n","        \"\"\"Load data in its raw form.\"\"\"\n","        data = {}\n","        if path_to_load.is_dir():\n","            files_to_load = get_paths(path_to_load)\n","            for file_path in files_to_load:\n","                is_required = file_path.stem in self.required_files\n","                is_required_roi = file_path.stem in self.full_roi_list\n","                if is_required or is_required_roi:\n","                    data[file_path.stem] = load_file(file_path)\n","        else:\n","            data[self.mode_name] = load_file(path_to_load)\n","\n","        return data\n","\n","    def shape_data(self, key: str, data: dict) -> NDArray:\n","        \"\"\"Shapes into form that is amenable to tensorflow and other deep learning packages.\"\"\"\n","\n","        shaped_data = np.zeros(self.required_files[key])\n","\n","        if key == \"structure_masks\":\n","            for roi_idx, roi in enumerate(self.full_roi_list):\n","                if roi in data.keys():\n","                    np.put(shaped_data, self.num_rois * data[roi] + roi_idx, int(1))\n","        elif key == \"possible_dose_mask\":\n","            np.put(shaped_data, data[key], int(1))\n","        elif key == \"voxel_dimensions\":\n","            shaped_data = data[key]\n","        else:\n","            np.put(shaped_data, data[key][\"indices\"], data[key][\"data\"])\n","\n","        return shaped_data\n"]},{"cell_type":"markdown","metadata":{"id":"HJXTpkn2WZ17"},"source":["Dose Evalution"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"tHzWsmNoWZ18"},"outputs":[],"source":["from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from numpy.typing import NDArray\n","\n","class DoseEvaluator:\n","    \"\"\"Evaluate a full dose distribution against the reference dose on the OpenKBP competition metrics\"\"\"\n","\n","    def __init__(self, reference_data_loader: DataLoader, prediction_loader: Optional[DataLoader] = None):\n","        self.reference_data_loader = reference_data_loader\n","        self.prediction_loader = prediction_loader\n","\n","        # Initialize objects for later\n","        self.reference_batch: Optional[DataBatch] = None\n","        self.prediction_batch: Optional[DataBatch] = None\n","\n","        # Define evaluation metrics for each roi\n","        oar_dvh_metrics = {oar: [\"D_0.1_cc\", \"mean\"] for oar in self.reference_data_loader.rois[\"oars\"]}\n","        target_dvh_metrics = {target: [\"D_99\", \"D_95\", \"D_1\"] for target in self.reference_data_loader.rois[\"targets\"]}\n","        self.all_dvh_metrics = oar_dvh_metrics | target_dvh_metrics\n","\n","        # Make data frames to cache evaluation metrics\n","        metric_columns = [(m, roi) for roi, metrics in self.all_dvh_metrics.items() for m in metrics]\n","        self.dose_errors = pd.Series(index=self.reference_data_loader.patient_id_list, data=None, dtype=float)\n","        self.dvh_metric_differences_df = pd.DataFrame(index=self.reference_data_loader.patient_id_list, columns=metric_columns)\n","        self.reference_dvh_metrics_df = self.dvh_metric_differences_df.copy()\n","        self.prediction_dvh_metrics_df = self.dvh_metric_differences_df.copy()\n","\n","    def evaluate(self):\n","        \"\"\"Calculate the  dose and DVH scores for the \"new_dose\" relative to the \"reference_dose\"\"\"\n","        if not self.reference_data_loader.patient_paths:\n","            raise ValueError(\"No reference patient data was provided, so no metrics can be calculated\")\n","        if self.prediction_loader:\n","            Warning(\"No predicted dose loader was provided. Metrics were only calculated for the reference dose.\")\n","        self._set_data_loader_mode()\n","\n","        for self.reference_batch in self.reference_data_loader.get_batches():\n","            self.reference_dvh_metrics_df = self._calculate_dvh_metrics(self.reference_dvh_metrics_df, self.reference_dose)\n","\n","            self.prediction_batch = self.prediction_loader.get_patients([self.patient_id]) if self.prediction_loader else None\n","            if self.predicted_dose is not None:\n","                patient_dose_error = np.sum(np.abs(self.reference_dose - self.predicted_dose)) / np.sum(self.possible_dose_mask)\n","                self.dose_errors[self.patient_id] = patient_dose_error\n","                self.prediction_dvh_metrics_df = self._calculate_dvh_metrics(self.prediction_dvh_metrics_df, self.predicted_dose)\n","\n","    def get_scores(self) -> tuple[NDArray, NDArray]:\n","        dose_score = np.nanmean(self.dose_errors)\n","        dvh_errors = np.abs(self.reference_dvh_metrics_df - self.prediction_dvh_metrics_df)\n","        dvh_score = np.nanmean(dvh_errors.values)\n","        return dose_score, dvh_score\n","\n","    def _set_data_loader_mode(self) -> None:\n","        self.reference_data_loader.set_mode(\"evaluation\")\n","        if self.prediction_loader:\n","            self.prediction_loader.set_mode(\"predicted_dose\")\n","\n","    def _calculate_dvh_metrics(self, metric_df: pd.DataFrame, dose: NDArray) -> pd.DataFrame:\n","        \"\"\"\n","        Calculate the DVH values that were used to evaluate submissions in the competition.\n","        :param metric_df: A DataFrame with columns indexed by the metric name and the structure name\n","        :param dose: the dose to be evaluated\n","        :return: the same metric_df that is input, but now with the metrics for the provided dose\n","        \"\"\"\n","        voxels_within_tenths_cc = np.maximum(1, np.round(100 / self.voxel_size))\n","        for roi in self.reference_data_loader.full_roi_list:\n","            roi_mask = self.get_roi_mask(roi)\n","            if roi_mask is None:\n","                continue  # Skip over ROIs when the ROI is missing (i.e., not contoured)\n","            roi_dose = dose[roi_mask]\n","            for metric in self.all_dvh_metrics[roi]:\n","                if metric == \"D_0.1_cc\":\n","                    roi_size = len(roi_dose)\n","                    fractional_volume_to_evaluate = 100 - voxels_within_tenths_cc / roi_size * 100\n","                    metric_value = np.percentile(roi_dose, fractional_volume_to_evaluate)\n","                elif metric == \"mean\":\n","                    metric_value = roi_dose.mean()\n","                elif metric == \"D_99\":\n","                    metric_value = np.percentile(roi_dose, 1)\n","                elif metric == \"D_95\":\n","                    metric_value = np.percentile(roi_dose, 5)\n","                elif metric == \"D_1\":\n","                    metric_value = np.percentile(roi_dose, 99)\n","                else:\n","                    raise ValueError(f\"Metrics {metric} is not supported.\")\n","                metric_df.at[self.patient_id, (metric, roi)] = metric_value\n","\n","        return metric_df\n","\n","    def get_roi_mask(self, roi_name: str) -> Optional[NDArray]:\n","        roi_index = self.reference_batch.get_index_structure_from_structure(roi_name)\n","        mask = self.reference_batch.structure_masks[:, :, :, :, roi_index].astype(bool)\n","        flat_mask = mask.flatten()\n","        return flat_mask if any(flat_mask) else None\n","\n","    @property\n","    def patient_id(self) -> str:\n","        patient_id, *_ = self.reference_batch.patient_list if self.reference_batch.patient_list else [None]\n","        return patient_id\n","\n","    @property\n","    def voxel_size(self) -> NDArray:\n","        return np.prod(self.reference_batch.voxel_dimensions)\n","\n","    @property\n","    def possible_dose_mask(self) -> NDArray:\n","        return self.reference_batch.possible_dose_mask\n","\n","    @property\n","    def reference_dose(self) -> NDArray:\n","        return self.reference_batch.dose.flatten()\n","\n","    @property\n","    def predicted_dose(self) -> NDArray:\n","        return self.prediction_batch.predicted_dose.flatten()\n"]},{"cell_type":"markdown","metadata":{"id":"mWVygermWZ18"},"source":["Network Achitecture"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"frbip-3RWZ18"},"outputs":[],"source":["from typing import Optional\n","from keras.layers import Activation, AveragePooling3D, Conv3D, Conv3DTranspose, Input, LeakyReLU, SpatialDropout3D, concatenate, MaxPooling3D, Dense, UpSampling3D\n","from keras.layers import BatchNormalization\n","from keras.models import Model\n","\n","class DefineDoseFromCT:\n","\n","    def __init__(\n","        self,\n","        data_shapes: DataShapes,\n","        gen_optimizer,\n","    ):\n","        self.data_shapes = data_shapes\n","        self.gen_optimizer = gen_optimizer\n","\n","    def make_convolution_block(self, x, num_filters: int):\n","        x = Conv3D(num_filters, (3,3,3), padding='same')(x)\n","        x = BatchNormalization(momentum=0.99, epsilon=1e-3)(x)\n","        x = LeakyReLU(alpha=0)(x)\n","        x = Conv3D(num_filters, (3,3,3), padding='same')(x)\n","        x = BatchNormalization(momentum=0.99, epsilon=1e-3)(x)\n","        x = LeakyReLU(alpha=0)(x)\n","        return x\n","\n","    def make_upsamle_block(self, x, num_filters: int, skip_x):\n","        if skip_x is not None:\n","            x = concatenate([x, skip_x])\n","        x = Conv3D(num_filters, (3, 3, 3), padding='same')(x)\n","        x = BatchNormalization(momentum=0.99, epsilon=1e-3)(x)\n","        x = LeakyReLU(alpha=0)(x)\n","        x = Conv3D(num_filters, (3, 3, 3), padding='same')(x)\n","        x = BatchNormalization(momentum=0.99, epsilon=1e-3)(x)\n","        x = LeakyReLU(alpha=0)(x)\n","        return x\n","\n","    def define_generator(self) -> Model:\n","\n","        ct_image = Input(self.data_shapes.ct)\n","        roi_masks = Input(self.data_shapes.structure_masks)\n","\n","        x = concatenate([ct_image, roi_masks])\n","        conv1 = self.make_convolution_block(x, 32)\n","        pool1 = MaxPooling3D(pool_size=(2,2,2))(conv1)\n","\n","        conv2 = self.make_convolution_block(pool1, 64)\n","        pool2 = MaxPooling3D(pool_size=(2,2,2))(conv2)\n","\n","        conv3 = self.make_convolution_block(pool2, 128)\n","        pool3 = MaxPooling3D(pool_size=(2,2,2))(conv3)\n","\n","        conv4 = self.make_convolution_block(pool3, 256)\n","\n","        up5 = UpSampling3D(size=(2,2,2))(conv4)\n","        conv5 = self.make_upsamle_block(up5, 128, conv3)\n","\n","        up6 = UpSampling3D(size=(2,2,2))(conv5)\n","        conv6 = self.make_upsamle_block(up6, 64, conv2)\n","\n","        up7 = UpSampling3D(size=(2,2,2))(conv6)\n","        conv7 = self.make_upsamle_block(up7, 2, conv1)\n","\n","        output = Conv3D(1, 1)(conv7)\n","        output = Activation(\"relu\")(output)\n","\n","        generator = Model(inputs=[ct_image, roi_masks], outputs=output, name=\"generator\")\n","        generator.compile(loss=\"mean_absolute_error\", optimizer=self.gen_optimizer)\n","        generator.summary()\n","        return generator\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"goQwE71yWZ18"},"outputs":[],"source":["import os\n","from pathlib import Path\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from keras.models import load_model\n","from keras.optimizers import Adam\n","\n","\n","class PredictionModel(DefineDoseFromCT):\n","    def __init__(self, data_loader: DataLoader, results_patent_path: Path, model_name: str, stage: str) -> None:\n","        \"\"\"\n","        :param data_loader: An object that loads batches of image data\n","        :param results_patent_path: The path at which all results and generated models will be saved\n","        :param model_name: The name of your model, used when saving and loading data\n","        :param stage: Identify stage of model development (train, validation, test)\n","        \"\"\"\n","        super().__init__(\n","            data_shapes=data_loader.data_shapes,\n","            gen_optimizer=Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999),\n","        )\n","\n","        # set attributes for data shape from data loader\n","        self.generator = None\n","        self.model_name = model_name\n","        self.data_loader = data_loader\n","        self.full_roi_list = data_loader.full_roi_list\n","\n","        # Define training parameters\n","        self.current_epoch = 0\n","        self.last_epoch = 200\n","\n","        # Make directories for data and models\n","        model_results_path = results_patent_path / model_name\n","        self.model_dir = model_results_path / \"models\"\n","        self.model_dir.mkdir(parents=True, exist_ok=True)\n","        self.prediction_dir = model_results_path / f\"{stage}-predictions\"\n","        self.prediction_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Make template for model path\n","        self.model_path_template = self.model_dir / \"epoch_\"\n","\n","    def train_model(self, epochs: int = 200, save_frequency: int = 5, keep_model_history: int = 2) -> None:\n","        \"\"\"\n","        :param epochs: the number of epochs the model will be trained over\n","        :param save_frequency: how often the model will be saved (older models will be deleted to conserve storage)\n","        :param keep_model_history: how many models are kept on a rolling basis (deletes older than save_frequency * keep_model_history epochs)\n","        \"\"\"\n","        self._set_epoch_start()\n","        self.last_epoch = epochs\n","        self.initialize_networks()\n","        if self.current_epoch == epochs:\n","            print(f\"The model has already been trained for {epochs}, so no more training will be done.\")\n","            return\n","        self.data_loader.set_mode(\"training_model\")\n","        for epoch in range(self.current_epoch, epochs):\n","            self.current_epoch = epoch\n","            print(f\"Beginning epoch {self.current_epoch}\")\n","            self.data_loader.shuffle_data()\n","\n","            for idx, batch in enumerate(self.data_loader.get_batches()):\n","                model_loss = self.generator.train_on_batch([batch.ct, batch.structure_masks], [batch.dose])\n","                print(f\"Model loss at epoch {self.current_epoch} batch {idx} is {model_loss:.3f}\")\n","\n","            self.manage_model_storage(save_frequency, keep_model_history)\n","\n","    def _set_epoch_start(self) -> None:\n","        all_model_paths = get_paths(self.model_dir, extension=\"h5\")\n","        for model_path in all_model_paths:\n","            *_, epoch_number = model_path.stem.split(\"epoch_\")\n","            if epoch_number.isdigit():\n","                self.current_epoch = max(self.current_epoch, int(epoch_number))\n","\n","    def initialize_networks(self) -> None:\n","        if self.current_epoch >= 1:\n","            self.generator = load_model(self._get_generator_path(self.current_epoch))\n","        else:\n","            self.generator = self.define_generator()\n","\n","    def manage_model_storage(self, save_frequency: int = 1, keep_model_history: Optional[int] = None) -> None:\n","        \"\"\"\n","        Manage the model storage while models are trained. Note that old models are deleted based on how many models the users has asked to keep.\n","        We overwrite old files (rather than deleting them) to ensure the Collab users don't fill up their Google Drive trash.\n","        :param save_frequency: how often the model will be saved (older models will be deleted to conserve storage)\n","        :param keep_model_history: how many models back are kept (older models will be deleted to conserve storage)\n","        \"\"\"\n","        effective_epoch_number = self.current_epoch + 1  # Epoch number + 1 because we're at the start of the next epoch\n","        if 0 < np.mod(effective_epoch_number, save_frequency) and effective_epoch_number != self.last_epoch:\n","            Warning(f\"Model at the end of epoch {self.current_epoch} was not saved because it is skipped when save frequency {save_frequency}.\")\n","            return\n","\n","        # The code below is clunky and was only included to bypass the Google Drive trash, which fills quickly with normal save/delete functions\n","        epoch_to_overwrite = effective_epoch_number - keep_model_history * (save_frequency or float(\"inf\"))\n","        if epoch_to_overwrite >= 0:\n","            initial_model_path = self._get_generator_path(epoch_to_overwrite)\n","            self.generator.save(initial_model_path)\n","            os.rename(initial_model_path, self._get_generator_path(effective_epoch_number))  # Helps bypass Google Drive trash\n","        else:  # Save via more conventional method because there is no model to overwrite\n","            self.generator.save(self._get_generator_path(effective_epoch_number))\n","\n","    def _get_generator_path(self, epoch: Optional[int] = None) -> Path:\n","        epoch = epoch or self.current_epoch\n","        return self.model_dir / f\"epoch_{epoch}.h5\"\n","\n","    def predict_dose(self, epoch: int = 1) -> None:\n","        \"\"\"Predicts the dose for the given epoch number\"\"\"\n","        self.generator = load_model(self._get_generator_path(epoch))\n","        os.makedirs(self.prediction_dir, exist_ok=True)\n","        self.data_loader.set_mode(\"dose_prediction\")\n","\n","        print(\"Predicting dose with generator.\")\n","        for batch in self.data_loader.get_batches():\n","            dose_pred = self.generator.predict([batch.ct, batch.structure_masks])\n","            dose_pred = dose_pred * batch.possible_dose_mask\n","            dose_pred = np.squeeze(dose_pred)\n","            dose_to_save = sparse_vector_function(dose_pred)\n","            dose_df = pd.DataFrame(data=dose_to_save[\"data\"].squeeze(), index=dose_to_save[\"indices\"].squeeze(), columns=[\"data\"])\n","            (patient_id,) = batch.patient_list\n","            dose_df.to_csv(\"{}/{}.csv\".format(self.prediction_dir, patient_id))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"O-QWn3-KWZ18","outputId":"4a5e4150-05d6-42a7-9ffe-162f44412c1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has already been trained for 100, so no more training will be done.\n","Predicting dose with generator.\n"]},{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:09,  9.40s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["2it [00:19,  9.96s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["3it [00:30, 10.36s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 11s 11s/step\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:42, 11.10s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 14s 14s/step\n"]},{"name":"stderr","output_type":"stream","text":["5it [00:58, 12.69s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["6it [01:08, 11.81s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["7it [01:19, 11.64s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 14s 14s/step\n"]},{"name":"stderr","output_type":"stream","text":["8it [01:34, 12.70s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 13s 13s/step\n"]},{"name":"stderr","output_type":"stream","text":["9it [01:49, 13.26s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 12s 12s/step\n"]},{"name":"stderr","output_type":"stream","text":["10it [02:02, 13.36s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["11it [02:12, 12.34s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 11s 11s/step\n"]},{"name":"stderr","output_type":"stream","text":["12it [02:24, 12.19s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["13it [02:35, 11.75s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["14it [02:46, 11.49s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["15it [02:57, 11.54s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["16it [03:09, 11.53s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["17it [03:20, 11.52s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["18it [03:31, 11.12s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["19it [03:40, 10.71s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["20it [03:51, 10.81s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["21it [04:02, 10.78s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["22it [04:13, 10.76s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["23it [04:24, 10.98s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 12s 12s/step\n"]},{"name":"stderr","output_type":"stream","text":["24it [04:37, 11.50s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 10s 10s/step\n"]},{"name":"stderr","output_type":"stream","text":["25it [04:48, 11.31s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["26it [04:58, 10.80s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["27it [05:08, 10.63s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 8s 8s/step\n"]},{"name":"stderr","output_type":"stream","text":["28it [05:17, 10.28s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["29it [05:27, 10.10s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["30it [05:37, 10.09s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["31it [05:46,  9.88s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["32it [05:56,  9.72s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["33it [06:06,  9.89s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["34it [06:17, 10.09s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["35it [06:26, 10.03s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 8s 8s/step\n"]},{"name":"stderr","output_type":"stream","text":["36it [06:36,  9.83s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 8s 8s/step\n"]},{"name":"stderr","output_type":"stream","text":["37it [06:45,  9.62s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 8s 8s/step\n"]},{"name":"stderr","output_type":"stream","text":["38it [06:55,  9.63s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 8s 8s/step\n"]},{"name":"stderr","output_type":"stream","text":["39it [07:04,  9.45s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 9s 9s/step\n"]},{"name":"stderr","output_type":"stream","text":["40it [07:15, 10.88s/it]\n","40it [00:38,  1.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["For this out-of-sample test on validation:\n","\tthe DVH score is 7.046\n","\tthe dose score is 5.284\n"]}],"source":["import shutil\n","from pathlib import Path\n","\n","if __name__ == \"__main__\":\n","\n","    prediction_name = \"baseline\"  # Name model to train and number of epochs to train it for\n","    test_time = False  # Only change this to True when the model has been fully tuned on the validation set\n","    num_epochs = 100 # This should probably be increased to 100-200 after your dry run\n","\n","    # Define project directories\n","    primary_directory = Path().resolve()  # directory where everything is stored\n","    provided_data_dir = primary_directory / \"provided-data\"\n","    training_data_dir = provided_data_dir / \"train-pats\"\n","    validation_data_dir = provided_data_dir / \"validation-pats\"\n","    testing_data_dir = provided_data_dir / \"test-pats\"\n","    results_dir = primary_directory / \"results_1\"  # where any data generated by this code (e.g., predictions, models) are stored\n","\n","    # Prepare the data directory\n","    training_plan_paths = get_paths(training_data_dir)  # gets the path of each plan's directory\n","\n","    # Train a model\n","    data_loader_train = DataLoader(training_plan_paths)\n","    dose_prediction_model_train = PredictionModel(data_loader_train, results_dir, prediction_name, \"train\")\n","    dose_prediction_model_train.train_model(num_epochs, save_frequency=1, keep_model_history=2)\n","\n","    # Define hold out set\n","    hold_out_data_dir = validation_data_dir if test_time is False else testing_data_dir\n","    stage_name, _ = hold_out_data_dir.stem.split(\"-\")\n","    hold_out_plan_paths = get_paths(hold_out_data_dir)\n","\n","    # Predict dose for the held out set\n","    data_loader_hold_out = DataLoader(hold_out_plan_paths)\n","    dose_prediction_model_hold_out = PredictionModel(data_loader_hold_out, results_dir, model_name=prediction_name, stage=stage_name)\n","    dose_prediction_model_hold_out.predict_dose(epoch=num_epochs)\n","\n","    # Evaluate dose metrics\n","    data_loader_hold_out_eval = DataLoader(hold_out_plan_paths)\n","    prediction_paths = get_paths(dose_prediction_model_hold_out.prediction_dir, extension=\"csv\")\n","    hold_out_prediction_loader = DataLoader(prediction_paths)\n","    dose_evaluator = DoseEvaluator(data_loader_hold_out_eval, hold_out_prediction_loader)\n","\n","    # print out scores if data was left for a hold out set\n","    if not data_loader_hold_out_eval.patient_paths:\n","        print(\"No patient information was given to calculate metrics\")\n","    else:\n","        dose_evaluator.evaluate()\n","        dvh_score, dose_score = dose_evaluator.get_scores()\n","        print(f\"For this out-of-sample test on {stage_name}:\\n\\tthe DVH score is {dvh_score:.3f}\\n\\tthe dose score is {dose_score:.3f}\")\n","\n","    # Zip dose to submit\n","    submission_dir = results_dir / \"submissions\"\n","    submission_dir.mkdir(exist_ok=True)\n","    shutil.make_archive(str(submission_dir / prediction_name), \"zip\", dose_prediction_model_hold_out.prediction_dir)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
